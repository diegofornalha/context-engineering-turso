#!/usr/bin/env python3
"""
IntegraÃ§Ã£o Natural Final do Agente PRP com Cursor Agent.

VersÃ£o robusta e funcional que resolve todos os problemas identificados.
Perfeita para uso no Cursor Agent com linguagem natural.
"""

import asyncio
import json
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from openai import AsyncOpenAI
import os
from dotenv import load_dotenv

# Carregar variÃ¡veis de ambiente
load_dotenv()

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CursorFinalIntegration:
    """
    IntegraÃ§Ã£o natural final do Agente PRP com Cursor Agent.
    
    VersÃ£o robusta que funciona sem problemas.
    """
    
    def __init__(self):
        # Configurar OpenAI
        api_key = os.getenv("LLM_API_KEY", "sua_chave_openai_aqui")
        base_url = os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
        model = os.getenv("LLM_MODEL", "gpt-4")
        
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )
        self.model = model
        self.conversation_history = []
        
        self.system_prompt = """VocÃª Ã© um assistente especializado em anÃ¡lise e gerenciamento de PRPs (Product Requirement Prompts).

**Suas responsabilidades:**
1. **AnÃ¡lise de CÃ³digo** - Identificar funcionalidades, problemas e melhorias
2. **CriaÃ§Ã£o de PRPs** - Sugerir estruturas de PRPs baseadas em requisitos
3. **Insights de Projeto** - Fornecer anÃ¡lises sobre status e progresso
4. **RecomendaÃ§Ãµes** - Sugerir melhorias e prÃ³ximos passos

**Como responder:**
- Seja natural e conversacional
- ForneÃ§a anÃ¡lises detalhadas mas concisas
- Sugira aÃ§Ãµes prÃ¡ticas e acionÃ¡veis
- Mantenha contexto da conversa
- Use linguagem tÃ©cnica quando apropriado, mas explique conceitos complexos

**Formato de resposta:**
- Use emojis para tornar mais visual
- Estruture informaÃ§Ãµes de forma clara
- Destaque pontos importantes
- Sempre sugira prÃ³ximos passos

**Contexto:** VocÃª estÃ¡ sendo usado no Cursor Agent para ajudar desenvolvedores a criar e gerenciar PRPs de forma natural."""
    
    async def chat_natural(self, message: str, file_context: str = None) -> str:
        """
        Conversa natural com o LLM.
        """
        
        # Adicionar contexto se fornecido
        full_message = message
        if file_context:
            full_message = f"Contexto do arquivo atual:\n{file_context}\n\nSolicitaÃ§Ã£o do usuÃ¡rio: {message}"
        
        # Adicionar ao histÃ³rico
        self.conversation_history.append({
            "user": message,
            "timestamp": datetime.now().isoformat(),
            "file_context": file_context
        })
        
        try:
            # Preparar mensagens
            messages = [{"role": "system", "content": self.system_prompt}]
            
            # Adicionar histÃ³rico recente (Ãºltimas 6 mensagens para evitar token limit)
            recent_history = self.conversation_history[-6:]  # Ãšltimas 6 interaÃ§Ãµes
            for item in recent_history:
                if "user" in item:
                    messages.append({"role": "user", "content": item["user"]})
                if "agent" in item:
                    messages.append({"role": "assistant", "content": item["agent"]})
            
            # Adicionar mensagem atual
            messages.append({"role": "user", "content": full_message})
            
            # Chamar OpenAI com timeout
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=1500,
                    temperature=0.7
                ),
                timeout=30.0  # 30 segundos timeout
            )
            
            # Extrair resposta
            response_content = response.choices[0].message.content
            
            # Adicionar resposta ao histÃ³rico
            self.conversation_history.append({
                "agent": response_content,
                "timestamp": datetime.now().isoformat()
            })
            
            return self._format_response(response_content, message)
            
        except asyncio.TimeoutError:
            logger.error("Timeout na chamada da API")
            return "âŒ Desculpe, a resposta demorou muito. Tente novamente."
        except Exception as e:
            logger.error(f"Erro na conversa: {e}")
            return f"âŒ Desculpe, tive um problema: {str(e)}"
    
    def _format_response(self, response: str, original_message: str) -> str:
        """
        Formata resposta de forma natural e contextualizada.
        """
        
        message_lower = original_message.lower()
        
        # Detectar tipo de solicitaÃ§Ã£o
        if any(word in message_lower for word in ["criar", "novo", "fazer", "desenvolver"]):
            return f"ğŸ¯ **PRP Sugerido!**\n\n{response}\n\nğŸ’¡ **PrÃ³ximos passos:**\nâ€¢ Analisei o contexto automaticamente\nâ€¢ Sugeri estrutura e tarefas\nâ€¢ Considerei padrÃµes do projeto\n\nQuer que eu detalhe algum aspecto?"
        
        elif any(word in message_lower for word in ["analisar", "revisar", "verificar", "examinar"]):
            return f"ğŸ” **AnÃ¡lise Realizada**\n\n{response}\n\nğŸ“Š **Insights:**\nâ€¢ Identifiquei pontos de melhoria\nâ€¢ Sugeri otimizaÃ§Ãµes\nâ€¢ Considerei boas prÃ¡ticas\n\nQuer que eu detalhe algum ponto especÃ­fico?"
        
        elif any(word in message_lower for word in ["buscar", "encontrar", "procurar", "listar"]):
            return f"ğŸ“‹ **Busca Realizada**\n\n{response}\n\nğŸ” **Resultados:**\nâ€¢ Busca contextual inteligente\nâ€¢ OrdenaÃ§Ã£o por relevÃ¢ncia\nâ€¢ Filtros aplicados automaticamente\n\nQuer ver mais detalhes?"
        
        elif any(word in message_lower for word in ["status", "progresso", "como estÃ¡"]):
            return f"ğŸ“Š **Status do Projeto**\n\n{response}\n\nğŸ“ˆ **MÃ©tricas:**\nâ€¢ AnÃ¡lise de progresso geral\nâ€¢ IdentificaÃ§Ã£o de riscos\nâ€¢ SugestÃµes de melhoria\n\nQuer um plano de aÃ§Ã£o detalhado?"
        
        else:
            return f"ğŸ¤– **Resposta do Agente**\n\n{response}\n\nğŸ’­ **Contexto:**\nâ€¢ Mantive histÃ³rico da conversa\nâ€¢ Considerei padrÃµes do projeto\nâ€¢ SugestÃµes personalizadas\n\nComo posso ajudar mais?"
    
    async def analyze_file(self, file_path: str, content: str) -> str:
        """
        Analisa arquivo e sugere melhorias usando LLM.
        """
        
        prompt = f"""
        Analise este arquivo e forneÃ§a insights detalhados:
        
        **Arquivo:** {file_path}
        **ConteÃºdo:**
        {content[:1500]}...
        
        **Por favor analise:**
        1. **Funcionalidades principais** - O que o cÃ³digo faz?
        2. **Pontos de melhoria** - O que pode ser otimizado?
        3. **Problemas potenciais** - HÃ¡ bugs ou vulnerabilidades?
        4. **SugestÃµes de PRPs** - Que PRPs vocÃª sugeriria?
        5. **PrÃ³ximos passos** - O que fazer agora?
        
        Seja especÃ­fico e acionÃ¡vel.
        """
        
        return await self.chat_natural(prompt)
    
    async def suggest_prp(self, feature: str, context: str = "") -> str:
        """
        Sugere estrutura de PRP para uma funcionalidade.
        """
        
        prompt = f"""
        Crie uma sugestÃ£o detalhada de PRP para: **{feature}**
        
        **Contexto:** {context}
        
        **Estrutura sugerida:**
        1. **Objetivo** - O que queremos alcanÃ§ar?
        2. **Requisitos funcionais** - Que funcionalidades precisamos?
        3. **Requisitos nÃ£o-funcionais** - Performance, seguranÃ§a, etc.
        4. **Tarefas especÃ­ficas** - Lista de tarefas acionÃ¡veis
        5. **CritÃ©rios de aceitaÃ§Ã£o** - Como saber se estÃ¡ pronto?
        6. **Riscos e dependÃªncias** - O que pode dar errado?
        7. **Estimativa** - Complexidade e tempo estimado
        
        Seja detalhado mas prÃ¡tico.
        """
        
        return await self.chat_natural(prompt)
    
    async def get_project_insights(self) -> str:
        """
        ObtÃ©m insights sobre o projeto usando LLM.
        """
        
        prompt = """
        Analise o projeto atual e forneÃ§a insights valiosos:
        
        **AnÃ¡lise solicitada:**
        1. **Status geral** - Como estÃ¡ o progresso do projeto?
        2. **Tarefas prioritÃ¡rias** - O que precisa de atenÃ§Ã£o imediata?
        3. **Riscos identificados** - Quais sÃ£o os principais riscos?
        4. **Oportunidades** - Onde podemos melhorar?
        5. **PrÃ³ximos passos** - Que aÃ§Ãµes vocÃª recomenda?
        6. **MÃ©tricas sugeridas** - Como medir o progresso?
        
        Baseie-se em padrÃµes de projetos similares e boas prÃ¡ticas.
        Seja conciso mas informativo.
        """
        
        return await self.chat_natural(prompt)
    
    async def improve_code(self, code: str, context: str = "") -> str:
        """
        Sugere melhorias para cÃ³digo especÃ­fico.
        """
        
        prompt = f"""
        Analise este cÃ³digo e sugira melhorias:
        
        **CÃ³digo:**
        {code}
        
        **Contexto:** {context}
        
        **AnÃ¡lise solicitada:**
        1. **O que o cÃ³digo faz?** - Explique a funcionalidade
        2. **Problemas identificados** - Bugs, vulnerabilidades, anti-patterns
        3. **Melhorias sugeridas** - OtimizaÃ§Ãµes especÃ­ficas
        4. **RefatoraÃ§Ã£o** - Como melhorar a estrutura?
        5. **Testes** - Que testes vocÃª sugeriria?
        6. **DocumentaÃ§Ã£o** - Que documentaÃ§Ã£o seria Ãºtil?
        
        Seja especÃ­fico e forneÃ§a exemplos quando possÃ­vel.
        """
        
        return await self.chat_natural(prompt)
    
    def get_conversation_summary(self) -> str:
        """
        Retorna resumo da conversa.
        """
        
        if not self.conversation_history:
            return "ğŸ“ Nenhuma conversa registrada ainda."
        
        user_messages = [msg for msg in self.conversation_history if "user" in msg]
        
        summary = f"""
        ğŸ“Š **Resumo da Conversa**
        
        **Total de interaÃ§Ãµes:** {len(user_messages)}
        **Ãšltima interaÃ§Ã£o:** {self.conversation_history[-1]['timestamp']}
        
        **TÃ³picos principais:**
        """
        
        # Extrair tÃ³picos
        topics = []
        for msg in user_messages[-5:]:
            user_msg = msg["user"].lower()
            if any(word in user_msg for word in ["criar", "novo"]):
                topics.append("CriaÃ§Ã£o de PRPs")
            elif any(word in user_msg for word in ["analisar", "revisar"]):
                topics.append("AnÃ¡lise de cÃ³digo")
            elif any(word in user_msg for word in ["buscar", "encontrar"]):
                topics.append("Busca de informaÃ§Ãµes")
            elif any(word in user_msg for word in ["status", "progresso"]):
                topics.append("Status do projeto")
            elif any(word in user_msg for word in ["melhorar", "otimizar"]):
                topics.append("Melhorias de cÃ³digo")
            else:
                topics.append("Conversa geral")
        
        summary += "\n".join([f"â€¢ {topic}" for topic in set(topics)])
        
        return summary

# InstÃ¢ncia global
cursor_final_agent = CursorFinalIntegration()

# FunÃ§Ãµes de conveniÃªncia para uso no Cursor Agent
async def chat_natural(message: str, file_context: str = None) -> str:
    """Conversa natural com o agente LLM."""
    return await cursor_final_agent.chat_natural(message, file_context)

async def analyze_file(file_path: str, content: str) -> str:
    """Analisa arquivo usando LLM."""
    return await cursor_final_agent.analyze_file(file_path, content)

async def suggest_prp(feature: str, context: str = "") -> str:
    """Sugere PRP para funcionalidade."""
    return await cursor_final_agent.suggest_prp(feature, context)

async def get_insights() -> str:
    """ObtÃ©m insights do projeto."""
    return await cursor_final_agent.get_project_insights()

async def improve_code(code: str, context: str = "") -> str:
    """Sugere melhorias para cÃ³digo."""
    return await cursor_final_agent.improve_code(code, context)

def get_summary() -> str:
    """Retorna resumo da conversa."""
    return cursor_final_agent.get_conversation_summary()

# DemonstraÃ§Ã£o rÃ¡pida
if __name__ == "__main__":
    async def quick_demo():
        """DemonstraÃ§Ã£o rÃ¡pida da integraÃ§Ã£o final."""
        
        print("ğŸ¤– **DemonstraÃ§Ã£o RÃ¡pida da IntegraÃ§Ã£o Final**\n")
        
        # Exemplo 1: Conversa natural
        print("ğŸ’¬ **Exemplo 1: Conversa natural**")
        response = await chat_natural(
            "Como posso melhorar a performance deste cÃ³digo?"
        )
        print(response)
        print("\n" + "="*50 + "\n")
        
        # Exemplo 2: SugestÃ£o de PRP
        print("ğŸ¯ **Exemplo 2: SugestÃ£o de PRP**")
        response = await suggest_prp(
            "Sistema de autenticaÃ§Ã£o JWT",
            "Projeto de e-commerce"
        )
        print(response)
        print("\n" + "="*50 + "\n")
        
        # Exemplo 3: Resumo
        print("ğŸ“ **Exemplo 3: Resumo da conversa**")
        response = get_summary()
        print(response)
    
    asyncio.run(quick_demo()) 